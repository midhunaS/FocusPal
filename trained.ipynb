{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVA1XqF7N8Gk",
        "outputId": "4cf702b5-12ca-4325-9e0b-e26a4c86e385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def extract_frames(video_path, skip_frames=10):\n",
        "    frames = []\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    success, image = vidcap.read()\n",
        "    count = 0\n",
        "\n",
        "    while success:\n",
        "        if count % skip_frames == 0:\n",
        "            # Resize to 224x224 immediately to save memory\n",
        "            image = cv2.resize(image, (224, 224))\n",
        "            frames.append(image)\n",
        "        success, image = vidcap.read()\n",
        "        count += 1\n",
        "\n",
        "    return frames\n",
        "\n",
        "# Load videos one by one and append frames to lists\n",
        "focus = []\n",
        "focus += extract_frames(\"/content/drive/My Drive/sampleVideos/jadefocused.mp4\", skip_frames=10)\n",
        "focus += extract_frames(\"/content/drive/My Drive/sampleVideos/mfocused.mp4\", skip_frames=10)\n",
        "focus += extract_frames(\"/content/drive/My Drive/sampleVideos/Afocused.mp4\", skip_frames=10)\n",
        "focus += extract_frames(\"/content/drive/My Drive/sampleVideos/agumofocused.mp4\", skip_frames=10)\n",
        "\n",
        "unfocus = []\n",
        "unfocus += extract_frames(\"/content/drive/My Drive/sampleVideos/jadeunfocused.mp4\", skip_frames=10)\n",
        "unfocus += extract_frames(\"/content/drive/My Drive/sampleVideos/munfocused.mp4\", skip_frames=10)\n",
        "unfocus += extract_frames(\"/content/drive/My Drive/sampleVideos/Aunfocused.mp4\", skip_frames=10)\n",
        "unfocus += extract_frames(\"/content/drive/My Drive/sampleVideos/agumounfocused.mp4\", skip_frames=10)\n",
        "\n"
      ],
      "metadata": {
        "id": "JUQf4RiLOTNr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, applications, losses\n",
        "import gc\n",
        "\n",
        "def image_formatting(image):\n",
        "    # converting pixel values (uint8) to float32 type\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    # normalizing the data to be in range of -1, +1\n",
        "    image = applications.resnet_v2.preprocess_input(image)\n",
        "    # resizing all images to a shape of 224x*224*3\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    return image\n",
        "\n",
        "y = []\n",
        "X = []\n",
        "for i in range(len(focus)):\n",
        "    image = image_formatting(focus[i])\n",
        "    X.append(np.array(image))\n",
        "    y.append(0)\n",
        "for i in range(len(unfocus)):\n",
        "    image = image_formatting(unfocus[i])\n",
        "    X.append(np.array(image))\n",
        "    y.append(1)\n",
        "\n",
        "del focus\n",
        "del unfocus\n",
        "gc.collect()\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "print(len(X))\n",
        "print(len(y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5XVA5S6P8PO",
        "outputId": "3438ff6b-20b7-4a4b-9718-ab3f05fafeb2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5754\n",
            "5754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "del X\n",
        "del y\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fg9OGKiOT9bU",
        "outputId": "1db1e08c-5d77-4ddb-b26b-4692f225ca0d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = applications.ResNet152V2(\n",
        "    weights='imagenet',\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False)"
      ],
      "metadata": {
        "id": "VS96iMegUIZW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "# add the entire base_model as \"first layer\"\n",
        "model.add(base_model)\n",
        "\n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "model.add(layers.Dense(256, activation=\"relu\"))\n",
        "\n",
        "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "# do not train the first layer (ResNet/base_model) of the model as it is already trained\n",
        "model.layers[0].trainable = False"
      ],
      "metadata": {
        "id": "vSP0Y1DAUQvE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "IVQMZFXvUTf6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)\n",
        "print(X_train.shape)\n",
        "history_fc = model.fit(X_train, y_train,\n",
        "                       batch_size=32, epochs=5,\n",
        "                       validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rOatlGsUZJF",
        "outputId": "fd00ae62-fd10-416a-b843-ecc586c3c87e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 ... 1 1 0]\n",
            "(5178, 224, 224, 3)\n",
            "Epoch 1/5\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 463ms/step - accuracy: 0.9591 - loss: 0.0946 - val_accuracy: 0.9981 - val_loss: 0.0042\n",
            "Epoch 2/5\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 288ms/step - accuracy: 0.9978 - loss: 0.0066 - val_accuracy: 1.0000 - val_loss: 0.0021\n",
            "Epoch 3/5\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 254ms/step - accuracy: 0.9982 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 7.9194e-04\n",
            "Epoch 4/5\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 284ms/step - accuracy: 1.0000 - loss: 5.7206e-04 - val_accuracy: 1.0000 - val_loss: 4.4605e-04\n",
            "Epoch 5/5\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 283ms/step - accuracy: 1.0000 - loss: 2.0291e-04 - val_accuracy: 1.0000 - val_loss: 3.3555e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy: \",test_accuracy*100,\"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rv5YlP0jUZw5",
        "outputId": "50a03ea7-e46a-4b62-fbe6-e461d1915106"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 199ms/step - accuracy: 0.9986 - loss: 0.0062\n",
            "Test Accuracy:  99.82638955116272 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(model, 'model/')"
      ],
      "metadata": {
        "id": "qLjpA948WbK7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive('saved_model', 'zip', 'model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZK1DrvkiWlnT",
        "outputId": "a6f93399-6a4c-4aaf-d23c-fba976218ff5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/saved_model.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp saved_model.zip \"/content/drive/My Drive/saved_model.zip\""
      ],
      "metadata": {
        "id": "rGV8rylnWr9D"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}